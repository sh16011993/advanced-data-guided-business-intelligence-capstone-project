{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/labeledTrainData.tsv'\n",
    "with open(path, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    next(reader)\n",
    "    pos_l = []\n",
    "    neg_l = []\n",
    "    for row in reader:\n",
    "        if row[1] == '1':\n",
    "            pos_l.append(row[2])\n",
    "        else:\n",
    "            neg_l.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_pos = pos_l[:4500]\n",
    "test_pos = pos_l[4500:5000]\n",
    "train_val_neg = neg_l[:4500]\n",
    "test_neg = neg_l[4500:5000]\n",
    "\n",
    "# train_val_pos = pos_l[:900]\n",
    "# test_pos = pos_l[900:1000]\n",
    "# train_val_neg = neg_l[:900]\n",
    "# test_neg = neg_l[900:1000]\n",
    "\n",
    "# train_val_pos = pos_l[:11250]\n",
    "# test_pos = pos_l[11250:]\n",
    "# train_val_neg = neg_l[:11250]\n",
    "# test_neg = neg_l[11250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_val_pos))\n",
    "# print(len(test_pos))\n",
    "# print(len(train_val_neg))\n",
    "# print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(l, vocab):\n",
    "    for doc in l:\n",
    "        tokens = clean_doc(doc)\n",
    "        vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80049\n",
      "[('br', 20759), ('The', 16056), ('movie', 14711), ('film', 13926), ('one', 8278), ('like', 6927), ('This', 5381), ('good', 5086), ('It', 4339), ('would', 4294), ('time', 4175), ('really', 4069), ('even', 3940), ('see', 3936), ('story', 3883), ('much', 3308), ('get', 3259), ('people', 3119), ('bad', 3021), ('great', 2988), ('made', 2841), ('well', 2835), ('films', 2810), ('make', 2803), ('movies', 2799), ('first', 2789), ('also', 2777), ('way', 2713), ('could', 2708), ('dont', 2669), ('think', 2630), ('But', 2596), ('characters', 2592), ('And', 2489), ('Its', 2430), ('character', 2359), ('seen', 2348), ('many', 2323), ('watch', 2256), ('never', 2252), ('two', 2251), ('plot', 2222), ('acting', 2207), ('little', 2127), ('In', 2118), ('know', 2116), ('best', 2073), ('life', 2064), ('ever', 2046), ('love', 2015)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(train_val_pos, vocab)\n",
    "process_docs(train_val_neg, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39193\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w', encoding='utf-8')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, './Data/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_2(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs_2(l, vocab):\n",
    "    documents = list()\n",
    "    for doc in l:\n",
    "        tokens = clean_doc_2(doc, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = './Data/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_docs = process_docs_2(train_val_pos, vocab)\n",
    "negative_docs = process_docs_2(train_val_neg, vocab)\n",
    "train_val_docs = positive_docs + negative_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting into Train and Validation Dataset\n",
    "train_pos = positive_docs[:4000]\n",
    "val_pos = positive_docs[4000:]\n",
    "\n",
    "train_neg = negative_docs[:4000]\n",
    "val_neg = negative_docs[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_pos))\n",
    "# print(len(val_pos))\n",
    "# print(len(train_neg))\n",
    "# print(len(val_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xtrain and ytrain\n",
    "train_docs = train_pos + train_neg\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(4000)] + [1 for _ in range(4000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xval and yval\n",
    "val_docs = val_pos + val_neg\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(val_docs)\n",
    "# pad sequences\n",
    "Xval = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "yval = array([0 for _ in range(500)] + [1 for _ in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xtest and ytest\n",
    "positive_docs = process_docs_2(test_pos, vocab)\n",
    "negative_docs = process_docs_2(test_neg, vocab)\n",
    "test_docs = positive_docs + negative_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(500)] + [1 for _ in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "raw_embedding = load_embedding('./Data/glove_6B/glove_6B_100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_model(n_filters, n_kernel_size, n_pool_size, learning_rate, act_fn):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    if act_fn == 'leakyRelu':\n",
    "        model.add(Conv1D(filters=n_filters, kernel_size=n_kernel_size))\n",
    "        model.add(LeakyReLU(alpha=0.05))\n",
    "    else:\n",
    "        model.add(Conv1D(filters=n_filters, kernel_size=n_kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=n_pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "n_epochs = [10]\n",
    "learning_rate = [0.001] #[0.1, 0.001, 0.0001]\n",
    "act_fn = ['relu', 'leakyRelu']\n",
    "n_pool_size = [2, 3, 4]\n",
    "n_filters = [64, 128, 256]\n",
    "n_kernel_size = [2, 3, 5]\n",
    "for epoch in n_epochs:\n",
    "    for a_fn in act_fn:\n",
    "        for l_rate in learning_rate:\n",
    "            for pool_size in n_pool_size:\n",
    "                for filters in n_filters:\n",
    "                    for kernel_size in n_kernel_size:\n",
    "                        print(\"Current Model: Epochs = {0}, a_fn = {1}, l_rate = {2}, pool_size = {3}, filter = {4}, kernel = {5}\".format(epoch, a_fn, l_rate, pool_size, filters, kernel_size))\n",
    "                        model = get_CNN_model(filters, kernel_size, pool_size, l_rate, a_fn)\n",
    "                        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                        model.fit(Xtrain, ytrain, epochs=epoch, verbose=2, validation_data = (Xval, yval), callbacks = [es])\n",
    "                        # Save the model\n",
    "                        model_name = \"{6}_epochs_{0}_a_fn_{1}_lrate_{2}_pool_size_{3}_filter_{4}_kernel_{5}\".format(epoch, a_fn, l_rate, pool_size, filters, kernel_size, \"CNN\")\n",
    "                        model.save(\"./CNN_models_medium/\" + model_name + \".h5\")\n",
    "                        # Save model config as json\n",
    "                        model_json = model.to_json()\n",
    "                        with open(\"./CNN_models_medium/json/\" + model_name + \".json\", \"w\") as json_file:\n",
    "                            json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.600000\n"
     ]
    }
   ],
   "source": [
    "# best model parameters\n",
    "pool_size = 2\n",
    "kernel_size = 2\n",
    "filters = 64\n",
    "l_rate = 0.001\n",
    "act_fn = 'leakyRelu'\n",
    "# Load the model\n",
    "large_model = load_model('./CNN_models_medium/CNN_epochs_10_a_fn_relu_lrate_0.001_pool_size_2_filter_256_kernel_2.h5')\n",
    "loss, acc = large_model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
