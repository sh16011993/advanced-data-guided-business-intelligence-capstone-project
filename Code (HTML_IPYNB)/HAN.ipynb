{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sh160\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum words per sentence\n",
    "MAX_WORDS = 100\n",
    "# Maximum sentences per doc\n",
    "MAX_SENT = 15\n",
    "# Max vocabulary size\n",
    "MAX_VOCAB = 20000\n",
    "# Dimension of GloVe\n",
    "GLOVE_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Load Kaggle IMDB Dataset\n",
    "# --------------------------------------------------\n",
    "\n",
    "dataset = pd.read_csv(\"./Data/labeledTrainData.tsv\", sep=\"\\t\")\n",
    "reviews = dataset[\"review\"].values\n",
    "sentiments = dataset[\"sentiment\"].values\n",
    "print(dataset.head())\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# Input matrix for Model, zero-pad as to not effect\n",
    "# predictions of attention mechanism\n",
    "x = np.zeros((len(reviews), MAX_SENT, MAX_WORDS), dtype=\"int32\")\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "\n",
    "    # Seperate each review into individual sentences\n",
    "    # https://www.nltk.org/api/nltk.tokenize.html\n",
    "    sentences = nltk.tokenize.sent_tokenize(review)\n",
    "    tokenized_sents = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    # Add padding\n",
    "    tokenized_sents = keras.preprocessing.sequence.pad_sequences(tokenized_sents, maxlen=MAX_WORDS)\n",
    "    padding = MAX_SENT - tokenized_sents.shape[0]\n",
    "\n",
    "    # No padding needed\n",
    "    if padding < 0:\n",
    "        tokenized_sents = tokenized_sents[0:MAX_SENT]\n",
    "    else:\n",
    "        # Add padding\n",
    "        tokenized_sents = np.pad(tokenized_sents, ((0, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Add to input matrix\n",
    "    x[i] = tokenized_sents[None, ...]\n",
    "\n",
    "# Convert sentiments for Keras\n",
    "y = keras.utils.to_categorical(sentiments)\n",
    "\n",
    "# Create test/train sets 20/80 split\n",
    "# x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)\n",
    "x_train = x[:4000]\n",
    "y_train = y[:4000]\n",
    "x_val = x[4000:4500]\n",
    "y_val = y[4000:4500]\n",
    "x_test = x[4500:5000]\n",
    "y_test = y[4500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Load word embeddings from GloVe\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Read in embeddings\n",
    "file = open(\"./Data/glove_6B/glove_6B_100d.txt\", \"r\", encoding = \"utf-8\")\n",
    "lines = file.readlines()\n",
    "embeddings = dict()\n",
    "for line in lines:\n",
    "    vals = line.split()\n",
    "    embeddings[vals[0]] = np.asarray(vals[1:], dtype=\"float32\")\n",
    "\n",
    "# Create weight matrix from embeddings\n",
    "embed_matrix = np.random.random((len(tokenizer.word_index) + 1, GLOVE_DIM))\n",
    "embed_matrix[0] = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embed_vec = embeddings.get(word)\n",
    "    if embed_vec is not None:\n",
    "        embed_matrix[i] = embed_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Create custom layer for HAN\n",
    "# https://keras.io/layers/writing-your-own-keras-layers/\n",
    "# --------------------------------------------------\n",
    "\n",
    "class han_attention_layer(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim=GLOVE_DIM, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(han_attention_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create trainable weight variables for this layer\n",
    "        dim = input_shape[1]\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(dim, self.output_dim),\n",
    "                                 initializer=keras.initializers.get(\"uniform\"),\n",
    "                                 trainable=True)\n",
    "\n",
    "        # Trainable weight\n",
    "        self.u = self.add_weight(name='output',\n",
    "                                 shape=(self.output_dim, 1),\n",
    "                                 initializer=keras.initializers.get(\"uniform\"),\n",
    "                                 trainable=True)\n",
    "\n",
    "        super(han_attention_layer, self).build(input_shape)\n",
    "\n",
    "    def get_att_weights(self, x):\n",
    "        u_tw = K.tanh(K.dot(x, self.W))\n",
    "        tw_stimulus = K.dot(u_tw, self.u)\n",
    "        tw_stimulus = K.reshape(tw_stimulus, (-1, tw_stimulus.shape[1]))\n",
    "        return K.softmax(tw_stimulus)\n",
    "\n",
    "    def call(self, x):\n",
    "        weights = self.get_att_weights(x)\n",
    "        weights = K.reshape(weights, (-1, weights.shape[1], 1))\n",
    "        weights = K.repeat_elements(weights, x.shape[-1], -1)\n",
    "        weighted_input = keras.layers.Multiply()([x, weights])\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Create HAN Model\n",
    "# --------------------------------------------------\n",
    "class han(keras.models.Model):\n",
    "    def __init__(self, max_words, max_sents, output_size, embed_matrix,\n",
    "                 word_encode_dim=200, sent_encode_dim=200,\n",
    "                 name=\"Hierarchical_Attention_Network\"):\n",
    "        self.max_words = max_words\n",
    "        self.max_sents = max_sents\n",
    "        self.output_size = output_size\n",
    "        self.embed_matrix = embed_matrix\n",
    "        self.word_encode_dim = word_encode_dim\n",
    "        self.sent_encode_dim = sent_encode_dim\n",
    "\n",
    "        in_tensor, out_tensor = self.build_network()\n",
    "\n",
    "        super(han, self).__init__(inputs=in_tensor, outputs=out_tensor, name=name)\n",
    "\n",
    "    def build_word_encoder(self, max_words, embed_matrix, encode_dim=200):\n",
    "        vocab_size = embed_matrix.shape[0]\n",
    "        embed_dim = embed_matrix.shape[1]\n",
    "        embed_layer = keras.layers.Embedding(vocab_size, embed_dim, weights=[embed_matrix],\n",
    "                                             input_length=max_words, trainable=False)\n",
    "        sent_input = keras.layers.Input(shape=(max_words,), dtype=\"int32\")\n",
    "        embed_sents = embed_layer(sent_input)\n",
    "        encode_sents = keras.layers.Bidirectional(keras.layers.GRU(int(encode_dim / 2)))(\n",
    "            embed_sents)\n",
    "        return keras.Model(inputs=[sent_input], outputs=[encode_sents], name=\"word_encoder\")\n",
    "\n",
    "    def build_sent_encoder(self, max_sents, summary_dim, encode_dim=200):\n",
    "        text_input = keras.layers.Input(shape=(max_sents, summary_dim))\n",
    "        encode_sents = keras.layers.Bidirectional(keras.layers.GRU(int(encode_dim / 2)))(\n",
    "            text_input)\n",
    "        return keras.Model(inputs=[text_input], outputs=[encode_sents], name=\"sentence_encoder\")\n",
    "\n",
    "    def build_network(self):\n",
    "        in_tensor = keras.layers.Input(shape=(self.max_sents, self.max_words))\n",
    "        word_encoder = self.build_word_encoder(self.max_words, self.embed_matrix, self.word_encode_dim)\n",
    "        word_rep = keras.layers.TimeDistributed(word_encoder, name=\"word_encoder\")(in_tensor)\n",
    "        sentence_rep = keras.layers.TimeDistributed(han_attention_layer(), name=\"word_attention\")(word_rep)\n",
    "        doc_rep = self.build_sent_encoder(self.max_sents, self.word_encode_dim, self.sent_encode_dim)(sentence_rep)\n",
    "        doc_summary = han_attention_layer(name=\"sentence_attention\")(doc_rep)\n",
    "        out_tensor = keras.layers.Dense(self.output_size, activation=\"softmax\", name=\"class_prediction\")(doc_summary)\n",
    "        return in_tensor, out_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "word_encoder (TimeDistribute (None, 15, 100)           8903600   \n",
      "_________________________________________________________________\n",
      "word_attention (TimeDistribu (None, 15, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sentence_encoder (Model)     (None, 100)               45300     \n",
      "_________________________________________________________________\n",
      "sentence_attention (han_atte (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "class_prediction (Dense)     (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 8,969,302\n",
      "Trainable params: 111,002\n",
      "Non-trainable params: 8,858,300\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "22500/22500 [==============================] - 875s 39ms/step - loss: 0.4548 - acc: 0.7814 - val_loss: 0.3506 - val_acc: 0.8456\n",
      "Epoch 2/10\n",
      "22500/22500 [==============================] - 872s 39ms/step - loss: 0.3498 - acc: 0.8482 - val_loss: 0.3241 - val_acc: 0.8620\n",
      "Epoch 3/10\n",
      " 6520/22500 [=======>......................] - ETA: 11:11 - loss: 0.3253 - acc: 0.8600"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Train / Test HAN Model\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Build\n",
    "han_model = han(MAX_WORDS, MAX_SENT, 2, embed_matrix, word_encode_dim=100, sent_encode_dim=100)\n",
    "han_model.summary()\n",
    "\n",
    "han_model.compile(optimizer=\"adagrad\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "han_model.fit(x_train, y_train, batch_size=20, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "n_epochs = [10]\n",
    "learning_rate = [0.001] #[0.1, 0.001, 0.0001]\n",
    "n_sent_encode_dim = [100, 200, 300]\n",
    "n_batch_size = [32, 64, 128]\n",
    "for epoch in n_epochs:\n",
    "        for l_rate in learning_rate:\n",
    "            for batch_size in n_batch_size:\n",
    "                for sent_encode_dim in n_sent_encode_dim:\n",
    "                    print(\"Current Model: Epochs = {0}, l_rate = {1}, batch_size = {2}, sent_encode_dim = {3}\".format(epoch, l_rate, batch_size, sent_encode_dim))\n",
    "                    han_model = han(MAX_WORDS, MAX_SENT, 2, embed_matrix, word_encode_dim=100, sent_encode_dim=100)\n",
    "                    han_model.compile(optimizer=\"adagrad\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "                    han_model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(x_val, y_val), verbose=1)\n",
    "                    # Save the model\n",
    "                    model_name = \"{4}_epochs_{0}_lrate_{1}_batch_size_{2}_sent_encode_dim_{3}\".format(epoch, l_rate, batch_size, sent_encode_dim, \"HAN\")\n",
    "                    model.save(\"./HAN_models_large/\" + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Sources:\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "# https://keras.io/layers/writing-your-own-keras-layers/\n",
    "# https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "# https://github.com/FlorisHoogenboom/keras-han-for-docla\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/\n",
    "# https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n",
    "# --------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
