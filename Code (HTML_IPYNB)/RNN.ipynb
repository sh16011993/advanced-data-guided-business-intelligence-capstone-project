{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM, Dropout \n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping\n",
    "import csv\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/labeledTrainData.tsv'\n",
    "with open(path, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    next(reader)\n",
    "    pos_l = []\n",
    "    neg_l = []\n",
    "    for row in reader:\n",
    "        if row[1] == '1':\n",
    "            pos_l.append(row[2])\n",
    "        else:\n",
    "            neg_l.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_pos = pos_l[:11250]\n",
    "test_pos = pos_l[11250:]\n",
    "train_val_neg = neg_l[:11250]\n",
    "test_neg = neg_l[11250:]\n",
    "\n",
    "# train_val_pos = pos_l[:900]\n",
    "# test_pos = pos_l[900:1000]\n",
    "# train_val_neg = neg_l[:900]\n",
    "# test_neg = neg_l[900:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_pos))\n",
    "# print(len(val_pos))\n",
    "# print(len(test_pos))\n",
    "# print(len(train_neg))\n",
    "# print(len(val_neg))\n",
    "# print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(l, vocab):\n",
    "    for doc in l:\n",
    "        tokens = clean_doc(doc)\n",
    "        vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130812\n",
      "[('br', 51650), ('The', 40001), ('movie', 37025), ('film', 33460), ('one', 20844), ('like', 17041), ('This', 13262), ('good', 12532), ('It', 10845), ('would', 10836), ('time', 10404), ('really', 10183), ('story', 9966), ('even', 9777), ('see', 9773), ('much', 8385), ('get', 8117), ('bad', 7630), ('people', 7616), ('great', 7444), ('made', 7100), ('first', 7072), ('well', 7021), ('also', 6918), ('films', 6863), ('make', 6836), ('movies', 6823), ('could', 6820), ('way', 6720), ('dont', 6593), ('characters', 6508), ('But', 6483), ('think', 6418), ('Its', 6051), ('And', 5996), ('seen', 5842), ('character', 5823), ('watch', 5656), ('many', 5639), ('two', 5570), ('never', 5544), ('acting', 5530), ('plot', 5466), ('little', 5389), ('know', 5353), ('In', 5320), ('best', 5146), ('show', 5141), ('love', 5132), ('life', 5117)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(train_val_pos, vocab)\n",
    "process_docs(train_val_neg, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62420\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w', encoding='utf-8')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, './Data/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_2(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs_2(l, vocab):\n",
    "    documents = list()\n",
    "    for doc in l:\n",
    "        tokens = clean_doc_2(doc, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = './Data/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_docs = process_docs_2(train_val_pos, vocab)\n",
    "negative_docs = process_docs_2(train_val_neg, vocab)\n",
    "train_val_docs = positive_docs + negative_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting into Train and Validation Dataset\n",
    "train_pos = positive_docs[:10000]\n",
    "val_pos = positive_docs[10000:]\n",
    "train_neg = negative_docs[:10000]\n",
    "val_neg = negative_docs[10000:]\n",
    "\n",
    "# train_pos = positive_docs[:800]\n",
    "# val_pos = positive_docs[800:]\n",
    "# train_neg = negative_docs[:800]\n",
    "# val_neg = negative_docs[800:]\n",
    "\n",
    "# train_pos = positive_docs[:4000]\n",
    "# val_pos = positive_docs[4000:]\n",
    "# train_neg = negative_docs[:4000]\n",
    "# val_neg = negative_docs[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xtrain and ytrain\n",
    "train_docs = train_pos + train_neg\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(10000)] + [1 for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xval and yval\n",
    "val_docs = val_pos + val_neg\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(val_docs)\n",
    "# pad sequences\n",
    "Xval = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "yval = array([0 for _ in range(1250)] + [1 for _ in range(1250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Xtest and ytest\n",
    "positive_docs = process_docs_2(test_pos, vocab)\n",
    "negative_docs = process_docs_2(test_neg, vocab)\n",
    "test_docs = positive_docs + negative_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(1250)] + [1 for _ in range(1250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "raw_embedding = load_embedding('./Data/glove_6B/glove_6B_100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, embeddings_initializer=Constant(embedding_vectors), input_length=max_length, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RNN_model(lstm_units, dense_units, n_dropout, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(dense_units, activation=\"relu\"))\n",
    "    model.add(Dropout(n_dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "n_epochs = [10]\n",
    "learning_rate = [0.001] #[0.1, 0.001, 0.0001]\n",
    "n_lstm_units = [16, 32, 64]\n",
    "n_dense_units = [20, 40, 60]\n",
    "n_dropout_rate = [0.2, 0.5, 0.8]\n",
    "for epoch in n_epochs:\n",
    "    for l_rate in learning_rate:\n",
    "        for lstm_units in n_lstm_units:\n",
    "            for dense_units in n_dense_units:\n",
    "                for dropout_rate in n_dropout_rate:\n",
    "                    print(\"Current Model: Epochs = {0}, l_rate = {1}, lstm_units = {2}, dense_units = {3}, dropout_rate = {4}\".format(epoch, l_rate, lstm_units, dense_units, dropout_rate))\n",
    "                    model = get_RNN_model(lstm_units, dense_units, dropout_rate, l_rate)\n",
    "                    # fit network\n",
    "                    model.fit(Xtrain, ytrain, epochs=epoch, verbose=1, validation_data = (Xval, yval), callbacks = [es])\n",
    "                    # Save the model\n",
    "                    model_name = \"{4}_epochs_{0}_lrate_{1}_lstm_units_{2}_dense_units_{3}_dropout_rate_{4}\".format(epoch, l_rate, lstm_units, dense_units, dropout_rate, \"RNN\")\n",
    "                    model.save(\"./RNN_models_large/\" + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 447s 22ms/step - loss: 0.4378 - acc: 0.7898 - val_loss: 0.3151 - val_acc: 0.8716\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 478s 24ms/step - loss: 0.2264 - acc: 0.9145 - val_loss: 0.2921 - val_acc: 0.8772\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 486s 24ms/step - loss: 0.1104 - acc: 0.9646 - val_loss: 0.3328 - val_acc: 0.8804\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 570s 29ms/step - loss: 0.0428 - acc: 0.9888 - val_loss: 0.4115 - val_acc: 0.8832\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 557s 28ms/step - loss: 0.0182 - acc: 0.9963 - val_loss: 0.5016 - val_acc: 0.8764\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 623s 31ms/step - loss: 0.0086 - acc: 0.9983 - val_loss: 0.5846 - val_acc: 0.8704\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 493s 25ms/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.7006 - val_acc: 0.8632\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 611s 31ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.6798 - val_acc: 0.8632\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 576s 29ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.7774 - val_acc: 0.8684\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 864s 43ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.8584 - val_acc: 0.8672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ba97c18d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model\n",
    "l_rate = 0.001\n",
    "lstm_units = 16\n",
    "dense_units = 20\n",
    "dropout_rate = 0.2\n",
    "model = get_RNN_model(lstm_units, dense_units, dropout_rate, l_rate)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1, validation_data = (Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
